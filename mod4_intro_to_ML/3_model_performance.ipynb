{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7e2a42",
   "metadata": {},
   "source": [
    "# Intro to ML: Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9e78",
   "metadata": {},
   "source": [
    "How do we know we can trust our models?\n",
    "\n",
    "Say we're working with a model that predicts weather a patient has cancer. If a model predicts that a pateint has cancer, we would want to know how sure the model is of this prediction. \n",
    "\n",
    "Models make predictions but our confidence/trust in a models ability to make **accurate** predictions comes from evaluating how well they perform on **practice (training) cases** and **new (test) cases**. \n",
    "\n",
    "This homework explores methods to assess the accuracy of model predictions. \n",
    "\n",
    "## Key idea üí°\n",
    "**‚ÄúA model is only as good as the data it‚Äôs trained on ‚Äî and as the metrics we use to judge it.‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d567d61",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "    <img src = \"https://www.explainxkcd.com/wiki/images/d/d7/flawed_data.png\" width = \"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2d927",
   "metadata": {},
   "source": [
    "Consider the following analogy: \n",
    "\n",
    "A students performance on a calculus test is directly related to the amount of time and the types of problems they practiced. It's best to evaluate a students calculus abilities using calculus problems! Don't use a geometry test to test abilities in calculus!\n",
    "\n",
    "In this analogy: \n",
    "\n",
    "1. model = student\n",
    "2. training data = the caluclus problems student practiced\n",
    "3. metrics to test model performance = calculus test \n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://dataedo-website.s3.amazonaws.com/cartoon/machine_learning.png?1654170935\" width = \"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07217c13",
   "metadata": {},
   "source": [
    "### Q: In your own words, and given the analogy above, define Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdbde4d",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb43c5",
   "metadata": {},
   "source": [
    "Hopefully this reinforces the idea of **Machine Learning**: an algorithm learning a relationship in the data so that it can make predictions when handed new data\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27148a53",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is defined as: \n",
    "\n",
    "$$\\text{Accuracy}=\\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$\n",
    "\n",
    "Though it's an intuitive first choice for a model performance metric it can be misleading when working with multi-class datasets. For example, predicting ‚Äúdoes not have cancer\" for everyone when 99% do not have cancer is not very helpful. We don't really know how well the model does at predicting in situations when someone does have cancer!\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.licdn.com/dms/image/v2/C5612AQGpdX8HSIAEeA/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1610873977828?e=2147483647&v=beta&t=gnytCodqB3H_WcT0Zz3NpDqHfbNNIL_OTctWWcB1iKE\" width = \"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36b4a5",
   "metadata": {},
   "source": [
    "## The Data Life Cycle in ML\n",
    "\n",
    "A schematic of the training (learning) and testing (evaluation) procedures for a ML model. \n",
    "<p align=\"left\">\n",
    "    <img src = \"https://www.machinelearningplus.com/wp-content/uploads/2022/12/train_test_split-procedure.jpg\" width = \"800\">\n",
    "</p>\n",
    "\n",
    "Imagine you‚Äôre studying for a test. If you only practice using the exact same questions that will be on the test, will you really know the material? Probably not ‚Äî you‚Äôd just memorize the answers.\n",
    "\n",
    "## Key idea üí°\n",
    "**We want our models to learn patterns in data, not memorize the training examples.**\n",
    "\n",
    "That‚Äôs why we divide our data into parts ‚Äî to test if the model can handle new, unseen examples.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74695ba",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "When we train a machine learning model:\n",
    "1. Training set: Used to teach the model patterns in the data.\n",
    "2. Testing set: Used to evaluate how well the model performs on new data.\n",
    "\n",
    "Typical we allocate 70‚Äì80% of the data for training and 20‚Äì30% for testing. \n",
    "\n",
    "### Generalization\n",
    "\n",
    "Generalization is a model‚Äôs ability to make accurate predictions on data it has never seen before. Models that don't generalize well fall into one of two categories, over or underfitting: \n",
    "\n",
    "| Type                       | Description                                                      | Example Behavior                            | \n",
    "| -------------------------- | ---------------------------------------------------------------- |  ------------------------------------------ |\n",
    "| Overfitting                | Overfitting\tModel is too complex, memorizes noise          | Perfect on training data, fails on new data        | \n",
    "| Underfitting               |  Model is too simple, doesn‚Äôt capture underlying pattern               | Straight line through nonlinear data | \t\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://miro.medium.com/v2/resize:fit:1400/1*pXJJTOS0f0dqgnlleP10aA.jpeg\" width = \"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14202e0",
   "metadata": {},
   "source": [
    "### Q: What are the pros and cons to splitting your data 80% train, 20% test vs. of 50% train, 50% test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23eb441",
   "metadata": {},
   "source": [
    "### A: \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36dff0",
   "metadata": {},
   "source": [
    "Below we train a classification algorithm to label patients as 'has cancer' or 'does not have cancer'. The train and testing accuracy is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0d7091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.958%\n",
      "Testing Accuracy: 0.956%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# split data into train (80%) and test (20%); random state for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# train a logistic regression model -- it's typically used to answer binary classification problems\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on both train and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred):.3f}%\")\n",
    "print(f\"Testing Accuracy: {accuracy_score(y_test, y_test_pred):.3f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31dd32",
   "metadata": {},
   "source": [
    "### Q: If you get > 95% accuracy in testing, does this alone tell us our model is great?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4130d",
   "metadata": {},
   "source": [
    "### A: \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eebb77",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c78a26",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "If a model predicts whether a patient has cancer or not, what does it mean if it‚Äôs 95%+ accurate? Is that good? We can't really tell! What we really want to know is: \n",
    "1. How accurate is the model at correctly predicting the pateint has cancer? \n",
    "2. How accurate is the model at correctly predicting the pateint does *not* have cancer?\n",
    "\n",
    "That‚Äôs where the classification report comes in ‚Äî it gives us a deeper look at model performance **by class**. The classification report summarizes several important metrics for each class in a classification problem. It shows how well the model identifies each category, it includes:\n",
    "\n",
    "| Metric    | Meaning                                                   | Formula                               |\n",
    "| --------- | --------------------------------------------------------- | ------------------------------------- |\n",
    "| Precision | Of all items predicted as class X, how many were correct? | $\\frac{TP}{TP + FP}$                |\n",
    "| Recall    | Of all true class X items, how many did we find?          | $\\frac{TP}{TP + FN}$                |\n",
    "| F1-Score  | Balance between precision and recall.                     | $2 \\times \\frac{P \\times R}{P + R}$ |\n",
    "| Support   | Number of true instances of each class.                   | Count                                 |\n",
    "\n",
    "Let Positive = 'does *not* have cancer' and Negative = 'does have cancer'\n",
    "- FP: False Positive\n",
    "    - ex: someone with cancer is told they do not have cancer\n",
    "- TP: True Positive \n",
    "    - ex: someone that does not have cancer is told they do not have cancer\n",
    "- FN: False Negative\n",
    "    - ex: someone without cancer is told they have cancer\n",
    "- TN: True Negative\n",
    "    - ex: someone with cancer is told they have cancer\n",
    "\n",
    "These Positive and Negative labels are summarized in the form of a **confusion matrix**, shown below: \n",
    "<p align=\"left\">\n",
    "    <img src = \"https://www.blog.trainindata.com/wp-content/uploads/2024/09/confusion-matrix-1.png\" width = \"500\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf0070",
   "metadata": {},
   "source": [
    "The confusion matrix for the algorithm we trained above is shown below. The matrix depicts the results of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d0f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[39  4]\n",
      " [ 1 70]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be8f0b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.91      0.94        43\n",
      "      benign       0.95      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred, target_names=load_breast_cancer().target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e0562",
   "metadata": {},
   "source": [
    "### Q: What could you say about the model we trained above if the precision and recall were both 1? \n",
    "Make quantitative arguments about the values of FN and FP to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9171406",
   "metadata": {},
   "source": [
    "### A: \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95373e25",
   "metadata": {},
   "source": [
    "### Q: Sometimes it's not possible to increase precision without decreasing recall (and vice-versa). Provide a conceptual argument for why this might be the case. \n",
    "Assume there's a way to force the model to maximize precision (or recall) upon the users request (there is). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb94458",
   "metadata": {},
   "source": [
    "### A: \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f845524",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## More Testing via Cross Validation  \n",
    "\n",
    "Sometimes, a single train/test split isn‚Äôt enough. Why? Because your model‚Äôs performance might depend on how the data happened to be split.\n",
    "\n",
    "Consider the following example: \n",
    "\n",
    "If you only take one practice test, it might not reflect your true skill. But if you take five different versions and average your scores, you‚Äôll get a better estimate of how you‚Äôll perform on the real exam.\n",
    "\n",
    "This is what **cross-validation** does!\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://miro.medium.com/1*GhKMAUmi4bfFiEwZCPlDsA.png\" width = \"700\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd319645",
   "metadata": {},
   "source": [
    "K-Fold Cross-Validation Psuedo Code: \n",
    "\n",
    "1. Split the data into K parts (folds).\n",
    "2. Train on K‚Äì1 folds, test on the remaining one.\n",
    "3. Repeat K times, each time using a different fold as the test set.\n",
    "4. Average the performance scores.\n",
    "\n",
    "Notice that cross validation uses *all data* for both training and testing (just not at the same time). This allows us to produce a more reliable estimate of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aa61f",
   "metadata": {},
   "source": [
    "### Types of Cross-Validation (CV)\n",
    "1. K-Fold CV: Standard method as described above.\n",
    "2. Stratified K-Fold: Keeps class proportions balanced.\n",
    "    - example: Given an image of a solid color, a machine learning model predicts the color of an image as \"orange\", \"purple\", or \"yellow\". Stratified K-fold splits up the data so that in each fold there is an equal amount of \"orange\", \"purple\", or \"yellow\" images for **both* training and testing. \n",
    "3. Leave-One-Out CV: Each sample is its own test set (expensive but precise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d460821",
   "metadata": {},
   "source": [
    "### Q: (Bonus, for Cool Nerds Only) If you're working with a classification model, why is it bad if your data is imbalanced? What would you need to do if your data is imbalanced but you still want to perform CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0bcd1c",
   "metadata": {},
   "source": [
    "### A: \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e9151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: [1.         1.         0.94444444 0.97142857 1.        ]\n",
      "Mean accuracy: 0.9831746031746033\n",
      "Standard deviation: 0.02230370548603213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# another type of algorithm commonly used for classification problems \n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation accuracies:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))\n",
    "print(\"Standard deviation:\", np.std(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97883052",
   "metadata": {},
   "source": [
    "### Q: I committed a cardinal sin when evaluating the performance of this algorithm. What was it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c6eff",
   "metadata": {},
   "source": [
    "### A:\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2dbeb",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:orange;\">Happy Halloween!</span>\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://static.wikia.nocookie.net/p__/images/1/1e/SpookleyTheSquarePumpkin.webp/revision/latest/thumbnail/width/360/height/360?cb=20250921145457&path-prefix=protagonist\" width = \"300\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
